{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b193403",
   "metadata": {},
   "source": [
    "# Introduction - Titanic Linear Regression\n",
    "\n",
    "Name: Gabriel Richards / gjrich\n",
    "\n",
    "Date: 5 Apr 2025\n",
    "\n",
    "Ever wondered how much people paid for their Titanic tickets? \n",
    "\n",
    "This lab uses the Titanic dataset to review regression modeling techniques. Previous work focused on classification (predicting survival), but we'll now predict the fare passengers paid for their journey. We'll review how  passenger attributes like age, family size, and other features correlate with ticket pricing. We'll also build multiple regression models including Linear Regression, Ridge, Elastic Net, and Polynomial Regression, and evaluate their performance using R², RMSE, and MAE to determine which features and models best predict passenger fares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4be2e3",
   "metadata": {},
   "source": [
    "## Python Package Imports\n",
    "\n",
    "Before anything else, we'll need to import the packages we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbba8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03923e",
   "metadata": {},
   "source": [
    "## Section 1. Import and Inspect the Data\n",
    "\n",
    "Load the Titanic dataset and confirm it’s structured correctly.\n",
    "\n",
    "\n",
    "Important: This code requires importing seaborn as sns and pandas. Our variable titanic holds a pandas DataFrame object. Know what imports are required for each bit of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4133987d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "      <td>Second</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>C</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0          0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1          1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2          1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3          1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4          0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "5          0       3    male   NaN      0      0   8.4583        Q   Third   \n",
       "6          0       1    male  54.0      0      0  51.8625        S   First   \n",
       "7          0       3    male   2.0      3      1  21.0750        S   Third   \n",
       "8          1       3  female  27.0      0      2  11.1333        S   Third   \n",
       "9          1       2  female  14.0      1      0  30.0708        C  Second   \n",
       "10         1       3  female   4.0      1      1  16.7000        S   Third   \n",
       "11         1       1  female  58.0      0      0  26.5500        S   First   \n",
       "12         0       3    male  20.0      0      0   8.0500        S   Third   \n",
       "13         0       3    male  39.0      1      5  31.2750        S   Third   \n",
       "14         0       3  female  14.0      0      0   7.8542        S   Third   \n",
       "15         1       2  female  55.0      0      0  16.0000        S  Second   \n",
       "16         0       3    male   2.0      4      1  29.1250        Q   Third   \n",
       "17         1       2    male   NaN      0      0  13.0000        S  Second   \n",
       "18         0       3  female  31.0      1      0  18.0000        S   Third   \n",
       "19         1       3  female   NaN      0      0   7.2250        C   Third   \n",
       "\n",
       "      who  adult_male deck  embark_town alive  alone  \n",
       "0     man        True  NaN  Southampton    no  False  \n",
       "1   woman       False    C    Cherbourg   yes  False  \n",
       "2   woman       False  NaN  Southampton   yes   True  \n",
       "3   woman       False    C  Southampton   yes  False  \n",
       "4     man        True  NaN  Southampton    no   True  \n",
       "5     man        True  NaN   Queenstown    no   True  \n",
       "6     man        True    E  Southampton    no   True  \n",
       "7   child       False  NaN  Southampton    no  False  \n",
       "8   woman       False  NaN  Southampton   yes  False  \n",
       "9   child       False  NaN    Cherbourg   yes  False  \n",
       "10  child       False    G  Southampton   yes  False  \n",
       "11  woman       False    C  Southampton   yes   True  \n",
       "12    man        True  NaN  Southampton    no   True  \n",
       "13    man        True  NaN  Southampton    no  False  \n",
       "14  child       False  NaN  Southampton    no   True  \n",
       "15  woman       False  NaN  Southampton   yes   True  \n",
       "16  child       False  NaN   Queenstown    no  False  \n",
       "17    man        True  NaN  Southampton   yes   True  \n",
       "18  woman       False  NaN  Southampton    no  False  \n",
       "19  woman       False  NaN    Cherbourg   yes   True  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Titanic dataset from seaborn and verify\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a051d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3d78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in embark_town: 2\n",
      "Number of empty strings in embark_town: 0\n",
      "Number of whitespace-only strings in embark_town: 0\n",
      "\n",
      "Rows with missing embark_town:\n",
      "     survived  pclass     sex   age  fare\n",
      "61          1       1  female  38.0  80.0\n",
      "829         1       1  female  62.0  80.0\n"
     ]
    }
   ],
   "source": [
    "# Check for null/NaN values\n",
    "null_count = titanic['embark_town'].isna().sum()\n",
    "print(f\"Number of null values in embark_town: {null_count}\")\n",
    "\n",
    "# Check for empty strings\n",
    "empty_string_count = (titanic['embark_town'] == '').sum()\n",
    "print(f\"Number of empty strings in embark_town: {empty_string_count}\")\n",
    "\n",
    "# Check for whitespace-only strings\n",
    "whitespace_count = (titanic['embark_town'].str.isspace()).sum()\n",
    "print(f\"Number of whitespace-only strings in embark_town: {whitespace_count}\")\n",
    "\n",
    "# Show rows with missing embark_town\n",
    "missing_embark = titanic[titanic['embark_town'].isna()]\n",
    "print(\"\\nRows with missing embark_town:\")\n",
    "print(missing_embark[['survived', 'pclass', 'sex', 'age', 'fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b29572",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration and Preparation\n",
    "\n",
    "Prepare the Titanic data for regression modeling. See the previous work.\n",
    "\n",
    "- Impute missing values for age using median\n",
    "- Drop rows with missing fare (or impute if preferred)\n",
    "- Create numeric variables (e.g., family_size from sibsp + parch + 1)\n",
    "- Optional - convert categorical features (e.g. sex, embarked) if you think they might help your prediction model. (We do not know relationships until we evaluate things.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3116dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing age values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of known ages\n",
    "age_mean = titanic['age'].mean()\n",
    "age_std = titanic['age'].std()\n",
    "\n",
    "# Count missing values\n",
    "age_null_count = titanic['age'].isnull().sum()\n",
    "\n",
    "# Generate random ages from normal distribution \n",
    "np.random.seed(307)  # For reproducibility\n",
    "age_random_values = np.random.normal(age_mean, age_std, age_null_count)\n",
    "\n",
    "# Apply age boundary constraints (0 to 85 years)\n",
    "age_random_values = np.clip(age_random_values, 0, 85)\n",
    "\n",
    "# Round ages to whole numbers\n",
    "age_random_values = np.round(age_random_values)\n",
    "\n",
    "# Create a mask for rows with missing age\n",
    "age_null_mask = titanic['age'].isnull()\n",
    "\n",
    "# Fill missing values with the random ages\n",
    "titanic.loc[age_null_mask, 'age'] = age_random_values\n",
    "\n",
    "# Round all ages in the dataset for consistency\n",
    "titanic['age'] = titanic['age'].round()\n",
    "\n",
    "# Verify no missing age values remain\n",
    "print(f\"Missing age values after imputation: {titanic['age'].isnull().sum()}\")\n",
    "\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94aaf30",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "Define multiple combinations of features to use as inputs to predict fare.\n",
    "\n",
    "Use unique names (X1, y1, X2, y2, etc.) so results are visible and can be compared at the same time. \n",
    "\n",
    "Remember the inputs, usually X, are a 2D array. The target is a 1D array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "636950a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. age\n",
    "X1 = titanic[['age']]\n",
    "y1 = titanic['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a5d3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2. family_size\n",
    "X2 = titanic[['family_size']]\n",
    "y2 = titanic['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3aae89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3. age, family_size\n",
    "X3 = titanic[['age', 'family_size']]\n",
    "y3 = titanic['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2beac5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4. Embark_town\n",
    "\n",
    "# Converting embark_town and who to numeric values\n",
    "# First, make a copy to avoid warnings about modifying the DataFrame\n",
    "titanic_prep = titanic.copy()\n",
    "\n",
    "# Convert embark_town to numeric\n",
    "titanic_prep['embark_town_numeric'] = titanic_prep['embark_town'].map({\n",
    "    'Southampton': 0, \n",
    "    'Cherbourg': 1, \n",
    "    'Queenstown': 2\n",
    "})\n",
    "\n",
    "# Fill any NaN values that might result from the mapping\n",
    "titanic_prep['embark_town_numeric'] = titanic_prep['embark_town_numeric'].fillna(0)  # Default to Southampton\n",
    "# Case 4. embark_town and who (converted to numeric)\n",
    "\n",
    "X4 = titanic_prep[['embark_town_numeric']]\n",
    "y4 = titanic_prep['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42f0d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 5. pclass\n",
    "X5 = titanic[['pclass']]\n",
    "y5 = titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42133586",
   "metadata": {},
   "source": [
    "### Reflection Questions:\n",
    "\n",
    "#### Why might these features affect a passenger's fare:\n",
    "- Embarkation town could affect fare due to varying pricing at each port \n",
    "- Additionally, certain ports might have catered to wealthier clientbases\n",
    "\n",
    "#### List all available features:\n",
    "- survived, pclass, sex, age, sibsp, parch, fare, embarked, class, who, adult_male, deck, embark_town, alive, alone\n",
    "\n",
    "#### Which other features could improve predictions and why:\n",
    "Pclass would likely have the strongest effect as ticket class was the primary fare determinant. Deck could indicate premium accommodation pricing. Family size (sibsp + parch) might reveal group pricing patterns. Age could show different pricing tiers for adults vs. children. Sex might indicate if there were gender-based pricing differences in that era.\n",
    "\n",
    "#### How many variables are in your Case 4:\n",
    "One variable: embark_town_numeric\n",
    "\n",
    "#### Which variable(s) did you choose for Case 4 and why do you feel those could make good inputs:\n",
    "I chose embark_town because geographic location could significantly impact ticket pricing. Each port had different market conditions, operating costs, and possibly different pricing strategies. Queenstown, Southampton, and Cherbourg likely had different base fare structures. This variable allows the model to detect if location-based pricing was a significant factor independent of passenger class or demographic characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abeacd2",
   "metadata": {},
   "source": [
    "## Section 4. Train a Regression Model (Linear Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97807223",
   "metadata": {},
   "source": [
    "### 4.1 Split the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bed58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=123)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=123)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=123)\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=123)\n",
    "\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cea6f6",
   "metadata": {},
   "source": [
    "### 4.2 Train and Evaluate Linear Regression Models (all 4 cases)\n",
    "\n",
    "\n",
    "We'll use a more concise approach - create each model and immediately call the fit() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73ee87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1 = LinearRegression().fit(X1_train, y1_train)\n",
    "lr_model2 = LinearRegression().fit(X2_train, y2_train)\n",
    "lr_model3 = LinearRegression().fit(X3_train, y3_train)\n",
    "lr_model4 = LinearRegression().fit(X4_train, y4_train)\n",
    "lr_model5 = LinearRegression().fit(X5_train, y5_train)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "\n",
    "y_pred_train1 = lr_model1.predict(X1_train)\n",
    "y_pred_test1 = lr_model1.predict(X1_test)\n",
    "\n",
    "y_pred_train2 = lr_model2.predict(X2_train)\n",
    "y_pred_test2 = lr_model2.predict(X2_test)\n",
    "\n",
    "y_pred_train3 = lr_model3.predict(X3_train)\n",
    "y_pred_test3 = lr_model3.predict(X3_test)\n",
    "\n",
    "y_pred_train4 = lr_model4.predict(X4_train)\n",
    "y_pred_test4 = lr_model4.predict(X4_test)\n",
    "\n",
    "y_pred_train5 = lr_model5.predict(X5_train)\n",
    "y_pred_test5 = lr_model5.predict(X5_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a13217",
   "metadata": {},
   "source": [
    "### 4.3 Report Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c6dbfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1: Training R²: 0.006091065737834356\n",
      "Case 1: Test R²: 0.005488036891723391\n",
      "Case 1: Test RMSE: 37.93215355924836\n",
      "Case 1: Test MAE: 24.981872979129427\n",
      "Case 2: Training R²: 0.049915792364760736\n",
      "Case 2: Test R²: 0.022231186110131973\n",
      "Case 2: Test RMSE: 37.6114940041967\n",
      "Case 2: Test MAE: 25.02534815941641\n",
      "Case 3: Training R²: 0.06616670833676175\n",
      "Case 3: Test R²: 0.04867533008141589\n",
      "Case 3: Test RMSE: 37.099398958668495\n",
      "Case 3: Test MAE: 24.126739164110607\n",
      "Case 4: Training R²: 0.003147509254272318\n",
      "Case 4: Test R²: 0.008497453278436762\n",
      "Case 4: Test RMSE: 37.874718285693156\n",
      "Case 4: Test MAE: 24.752302754209744\n",
      "Case 5: Training R²: 0.3005588037487471\n",
      "Case 5: Test R²: 0.3016017234169923\n",
      "Case 5: Test RMSE: 31.7873316928033\n",
      "Case 5: Test MAE: 20.653703671484056\n"
     ]
    }
   ],
   "source": [
    "# Case 1 Performance\n",
    "print(\"Case 1: Training R²:\", r2_score(y1_train, y_pred_train1))\n",
    "print(\"Case 1: Test R²:\", r2_score(y1_test, y_pred_test1))\n",
    "print(\"Case 1: Test RMSE:\", root_mean_squared_error(y1_test, y_pred_test1))\n",
    "print(\"Case 1: Test MAE:\", mean_absolute_error(y1_test, y_pred_test1))\n",
    "\n",
    "# Case 2 Performance\n",
    "print(\"Case 2: Training R²:\", r2_score(y2_train, y_pred_train2))\n",
    "print(\"Case 2: Test R²:\", r2_score(y2_test, y_pred_test2))\n",
    "print(\"Case 2: Test RMSE:\", root_mean_squared_error(y2_test, y_pred_test2))\n",
    "print(\"Case 2: Test MAE:\", mean_absolute_error(y2_test, y_pred_test2))\n",
    "\n",
    "# Case 3 Performance\n",
    "print(\"Case 3: Training R²:\", r2_score(y3_train, y_pred_train3))\n",
    "print(\"Case 3: Test R²:\", r2_score(y3_test, y_pred_test3))\n",
    "print(\"Case 3: Test RMSE:\", root_mean_squared_error(y3_test, y_pred_test3))\n",
    "print(\"Case 3: Test MAE:\", mean_absolute_error(y3_test, y_pred_test3))\n",
    "\n",
    "# Case 4 Performance\n",
    "print(\"Case 4: Training R²:\", r2_score(y4_train, y_pred_train4))\n",
    "print(\"Case 4: Test R²:\", r2_score(y4_test, y_pred_test4))\n",
    "print(\"Case 4: Test RMSE:\", root_mean_squared_error(y4_test, y_pred_test4))\n",
    "print(\"Case 4: Test MAE:\", mean_absolute_error(y4_test, y_pred_test4))\n",
    "\n",
    "# Case 5 Performance\n",
    "print(\"Case 5: Training R²:\", r2_score(y5_train, y_pred_train5))\n",
    "print(\"Case 5: Test R²:\", r2_score(y5_test, y_pred_test5))\n",
    "print(\"Case 5: Test RMSE:\", root_mean_squared_error(y5_test, y_pred_test5))\n",
    "print(\"Case 5: Test MAE:\", mean_absolute_error(y5_test, y_pred_test5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46101bc2",
   "metadata": {},
   "source": [
    "### Section 4 Reflection Questions\n",
    "\n",
    "#### Compare the train vs test results for each.\n",
    "\n",
    "While I did answer the questions for cases 1-4, it's worth saying that with extremely low R² values, the concepts of overfitting and underfitting become somewhat meaningless. For a model to meaningfully overfit or underfit, there should be an actual pattern in the data that can be captured. When R² values are near zero, they indicate that essentially no relationship exists between the predictors and the target variable, fare.\n",
    "\n",
    "1. **Did Case 1 overfit or underfit? Explain:**\n",
    "   Age -  underfit. Both training R² (0.0061) and test R² (0.0055) are extremely low and similar, indicating the model failed to capture meaningful patterns between age and fare.\n",
    "\n",
    "2. **Did Case 2 overfit or underfit? Explain:**\n",
    "   family_size - showed slight overfitting. The training R² (0.0499) is more than double the test R² (0.0222), indicating the model learned patterns in training data that didn't generalize well to test data. Still, both values are very low.\n",
    "\n",
    "3. **Did Case 3 overfit or underfit? Explain:**\n",
    "   age and family_size - showed slight overfitting as the training R² (0.0662) is higher than the test R² (0.0487). This captured more information than the first two cases, but still explains less than 5% of fare variance.\n",
    "\n",
    "4. **Did Case 4 overfit or underfit? Explain:**\n",
    "   embark_town - likely underfit, though it shows with test R² (0.0085) higher than training R² (0.0031). However the R² values suggest random variation in the data split rather than a true pattern.\n",
    "\n",
    "5. **Did Case 5 overfit or underfit? Explain:**\n",
    "   pclass - showed balanced fitting. I included it as a bonus sanity test since the correlations were so low on the other cases. The training R² (0.301) and test R² (0.302) are virtually identical, indicating good generalization. This model explains about 30% of fare variance, significantly better than all other cases.\n",
    "\n",
    "\n",
    "#### Adding Age\n",
    "\n",
    "1. **Did adding age improve the model:**\n",
    "   Yes, infinitesmally. Comparing Case 2 (family_size only, R²=0.022) to Case 3 (age and family_size, R²=0.049), adding age did double the test R² and reduced MAE from 25.03 to 24.13...but despite that, the correlation generally remained very low.\n",
    "\n",
    "2. **Propose a possible explanation:**\n",
    "   Age might effect ticket price because: \n",
    "   (1) different fare structures may have existed for children vs. adults, \n",
    "   (2) age may correlate with wealth that influenced cabin class selection \n",
    "   (3) families with young children might have required different accommodations. \n",
    "   \n",
    "   However, the low R² values suggest this relationship is still quite weak.\n",
    "\n",
    "\n",
    "#### Worst\n",
    "\n",
    "1. **Which case performed the worst:**\n",
    "   Case 1 - age\n",
    "\n",
    "2. **How do you know:**\n",
    "   It has the lowest test R² (0.0055), and the highest RMSE [37.93].\n",
    "\n",
    "3. **Do you think adding more training data would improve it (and why/why not):**\n",
    "   No. The problem isn't insufficient data, but that age alone has almost no relationship with fare (R² near zero). More data with the same weak predictor probably wouldn't improve performance.\n",
    "   \n",
    "\n",
    "#### Best\n",
    "\n",
    "1. **Which case performed the best:**\n",
    "   Case 5  - pclass\n",
    "\n",
    "2. **How do you know:**\n",
    "   It has much higher test R² (0.302) than the others. It also has the lowest RMSE (31.79) and MAE (20.65), indicating more accurate predictions.\n",
    "\n",
    "3. **Do you think adding more training data would improve it (and why/why not):**\n",
    "   No, I think not. The model already shows consistent generalization, nearly identical between train and test R². However adding different features like deck or combining with other features might likely yield better improvements than more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8c753",
   "metadata": {},
   "source": [
    "## Section 5. Compare Alternative Models\n",
    "In this section, we will take the best-performing case and explore other regression models.\n",
    "\n",
    "Choose Best Case to Continue\n",
    "Choose the best case model from the four cases. Use that model to continue to explore additional continuous prediction models. The following assumes that Case 1 was the best predictor  - this may not be the case. Adjust the code to use your best case model instead. \n",
    "\n",
    "Choosing Options\n",
    "When working with regression models, especially those with multiple input features, we may run into overfitting — where a model fits the training data too closely and performs poorly on new data. To prevent this, we can apply regularization.\n",
    "\n",
    "Regularization adds a penalty to the model’s loss function, discouraging it from using very large weights (coefficients). This makes the model simpler and more likely to generalize well to new data.\n",
    "\n",
    "In general: \n",
    "\n",
    "If the basic linear regression is overfitting, try Ridge.\n",
    "\n",
    "If you want the model to automatically select the most important features, try Lasso.\n",
    "\n",
    "If you want a balanced approach, try Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426f204",
   "metadata": {},
   "source": [
    "### 5.1 Ridge Regression (L2 penalty)\n",
    "Ridge Regression is a regularized version of linear regression that adds a penalty to large coefficient values. It uses the L2 penalty, which adds the sum of squared coefficients to the loss function.\n",
    "\n",
    "This \"shrinks\" the coefficients, reducing the model’s sensitivity to any one feature while still keeping all features in the model.\n",
    "\n",
    "Penalty term: L2 = sum of squared weights\n",
    "Effect: Shrinks weights, helps reduce overfitting, keeps all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cac5f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X5_train, y5_train)\n",
    "y_pred_ridge = ridge_model.predict(X5_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7929b",
   "metadata": {},
   "source": [
    "### 5.2 Elastic Net (L1 + L2 combined)\n",
    "Lasso Regression uses the L1 penalty, which adds the sum of absolute values of the coefficients to the loss function. Lasso can shrink some coefficients all the way to zero, effectively removing less important features. This makes it useful for feature selection.\n",
    "\n",
    "Penalty term: L1 = sum of absolute values of weights\n",
    "Effect: Can shrink some weights to zero (drops features), simplifies the model\n",
    "Elastic Net combines both L1 (Lasso) and L2 (Ridge) penalties. It balances the feature selection ability of Lasso with the stability of Ridge.\n",
    "\n",
    "We control the balance with a parameter called l1_ratio:\n",
    "\n",
    "If l1_ratio = 0, it behaves like Ridge\n",
    "If l1_ratio = 1, it behaves like Lasso\n",
    "Values in between mix both types\n",
    "Penalty term: α × (L1 + L2)\n",
    "Effect: Shrinks weights and can drop some features — flexible and powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d875cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5)\n",
    "elastic_model.fit(X5_train, y5_train)\n",
    "y_pred_elastic = elastic_model.predict(X5_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36deda3",
   "metadata": {},
   "source": [
    "### 5.3 Polynomial Regression\n",
    "Linear regression is a simple two dimensional relationship - a simple straight line. But we can test more complex relationships. Polynomial regression adds interaction and nonlinear terms to the model. Be careful here - higher-degree polynomials can easily overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5098a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the poly inputs\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X5_train)\n",
    "X5_test_poly = poly.transform(X5_test)\n",
    " \n",
    "\n",
    "# Use the poly inputs in the LR model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y5_train)\n",
    "y_pred_poly = poly_model.predict(X5_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e95eec",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Polynomial Cubic Fit (for 1 input feature)\n",
    "Choose a case with just one input feature and plot it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X5_test, y5_test, color='blue', label='Actual')\n",
    "plt.scatter(X5_test, y_pred_poly, color='red', label='Predicted (Poly)')\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression: Age vs pclass\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "260854f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear R²: 0.302\n",
      "Linear RMSE: 31.79\n",
      "Linear MAE: 20.65\n",
      "\n",
      "Ridge R²: 0.302\n",
      "Ridge RMSE: 31.77\n",
      "Ridge MAE: 20.64\n",
      "\n",
      "ElasticNet R²: 0.339\n",
      "ElasticNet RMSE: 30.92\n",
      "ElasticNet MAE: 19.93\n",
      "\n",
      "Polynomial R²: 0.336\n",
      "Polynomial RMSE: 30.99\n",
      "Polynomial MAE: 19.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def report(name, y_true, y_pred):\n",
    "    print(f\"{name} R²: {r2_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"{name} RMSE: {root_mean_squared_error(y_true, y_pred):.2f}\")\n",
    "    print(f\"{name} MAE: {mean_absolute_error(y_true, y_pred):.2f}\\n\")\n",
    "\n",
    "report(\"Linear\", y5_test, y_pred_test5)\n",
    "report(\"Ridge\", y5_test, y_pred_ridge)\n",
    "report(\"ElasticNet\", y5_test, y_pred_elastic)\n",
    "report(\"Polynomial\", y5_test, y_pred_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7395ebc",
   "metadata": {},
   "source": [
    "### 5.4 Reflections\n",
    "\n",
    "1. What patterns does the cubic model seem to capture:\n",
    "   - Model clusters them by each class (1, 2, and 3), with first class (1) having the highest fares, second class (2) having medium fares, and third class (3) having the lowest fares. The model creates a curve that attempts to fit these discrete class-based fare differences\n",
    "2. Where does it perform well or poorly:\n",
    "   - The model performs well at establishing the general relationship between class and fare (higher class = higher fare). It struggles with the variance in fares within each class, particularly in first class where the fare values range dramatically from $30 to nearly $250.\n",
    "3. Did the polynomial fit outperform linear regression:\n",
    "   - The polynomial fit (R² = 0.336, RMSE = 30.99, MAE = 19.25) slightly outperformed the linear regression model (R² = 0.302, RMSE = 31.79, MAE = 20.65). The ElasticNet model actually performed slightly better than both with R² = 0.339. The improvement is modest but consistent across all evaluation metrics.\n",
    "4. Where (on the graph or among which kinds of data points) does it fit best:  It would struggle most with first class passengers where there's tremendous fare variability. The ElasticNet's regularization helps it avoid overfitting to outliers in the first class, allowing it to create a more balanced fit across all classes while still capturing the essential relationship between passenger class and fare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb26d00",
   "metadata": {},
   "source": [
    "### 5.5 Visualize Higher Order Polynomial (for the same 1 input case)\n",
    "\n",
    "Use the same single input case as you visualized above, but use a higher degree polynomial (e.g. 4, 5, 6, 7, or 8). Plot the result. \n",
    "\n",
    "In a Markdown cell, tell us which option seems to work better - your initial cubic (3) or your higher order and why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple polynomial degrees using pclass as the input feature\n",
    "degrees = [3, 4, 6, 8]\n",
    "poly_models_pclass = {}\n",
    "y_preds_pclass = {}\n",
    "r2_scores_pclass = {}\n",
    "\n",
    "# Train models for each degree\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly_transformer = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly_deg = poly_transformer.fit_transform(X5_train)  # Using X5 (pclass) instead of X1 (age)\n",
    "    X_test_poly_deg = poly_transformer.transform(X5_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly_deg, y5_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test_poly_deg)\n",
    "    \n",
    "    # Store results\n",
    "    poly_models_pclass[degree] = model\n",
    "    y_preds_pclass[degree] = y_pred\n",
    "    r2_scores_pclass[degree] = r2_score(y5_test, y_pred)\n",
    "\n",
    "# Create plots for each degree\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, degree in enumerate(degrees, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(X5_test, y5_test, color='blue', alpha=0.6, label='Actual')\n",
    "    plt.scatter(X5_test, y_preds_pclass[degree], color=f'C{i}', alpha=0.6, label=f'Degree {degree}')\n",
    "    \n",
    "    # Create sorted data for a cleaner line plot\n",
    "    sorted_indices = X5_test.iloc[:, 0].argsort()\n",
    "    sorted_X = X5_test.iloc[sorted_indices]\n",
    "    sorted_y_pred = y_preds_pclass[degree][sorted_indices]\n",
    "    \n",
    "    # Draw a line connecting predictions\n",
    "    plt.plot(sorted_X, sorted_y_pred, color=f'C{i}', linestyle='-', alpha=0.7)\n",
    "    \n",
    "    plt.title(f\"Polynomial Regression (Degree {degree}): Pclass vs Fare\")\n",
    "    plt.xlabel(\"Passenger Class\")\n",
    "    plt.ylabel(\"Fare\")\n",
    "    plt.legend()\n",
    "    plt.xticks([1, 2, 3], ['First', 'Second', 'Third'])\n",
    "    \n",
    "    # Add R² value to plot\n",
    "    plt.annotate(f\"R² = {r2_scores_pclass[degree]:.4f}\", \n",
    "                 xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report performance metrics for each model\n",
    "print(\"Performance Metrics for Different Polynomial Degrees (Pclass):\\n\")\n",
    "for degree in degrees:\n",
    "    print(f\"Polynomial Degree {degree}:\")\n",
    "    print(f\"R²: {r2_scores_pclass[degree]:.4f}\")\n",
    "    print(f\"RMSE: {root_mean_squared_error(y5_test, y_preds_pclass[degree]):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y5_test, y_preds_pclass[degree]):.2f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c906f",
   "metadata": {},
   "source": [
    "### P-class Polynomial Regression Review\n",
    "\n",
    "R^2 values are good compared to other cases, but clearly changing the degree makes no difference and the visualizations are unclear. Let's try combining P-class with family size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature sets\n",
    "X_pclass = titanic[['pclass']]\n",
    "X_combined = titanic[['pclass', 'family_size']]\n",
    "\n",
    "# Split the data\n",
    "X_pclass_train, X_pclass_test, y_pclass_train, y_pclass_test = train_test_split(\n",
    "    X_pclass, titanic['fare'], test_size=0.2, random_state=123)\n",
    "X_combined_train, X_combined_test, y_combined_train, y_combined_test = train_test_split(\n",
    "    X_combined, titanic['fare'], test_size=0.2, random_state=123)\n",
    "\n",
    "# Train linear regression models\n",
    "lr_pclass = LinearRegression()\n",
    "lr_pclass.fit(X_pclass_train, y_pclass_train)\n",
    "y_pred_pclass = lr_pclass.predict(X_pclass_test)\n",
    "\n",
    "lr_combined = LinearRegression()\n",
    "lr_combined.fit(X_combined_train, y_combined_train)\n",
    "y_pred_combined = lr_combined.predict(X_combined_test)\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Pclass only (2D plot)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(X_pclass_test, y_pclass_test, color='blue', alpha=0.6, marker='o', label='Actual')\n",
    "ax1.scatter(X_pclass_test, y_pred_pclass, color='red', alpha=0.6, marker='^', label='Predicted')\n",
    "\n",
    "# Create sorted data for a cleaner line plot\n",
    "sorted_indices = X_pclass_test.iloc[:, 0].argsort()\n",
    "sorted_X = X_pclass_test.iloc[sorted_indices]\n",
    "sorted_y_pred = y_pred_pclass[sorted_indices]\n",
    "\n",
    "# Draw a line connecting predictions\n",
    "ax1.plot(sorted_X, sorted_y_pred, color='red', linestyle='-', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Passenger Class')\n",
    "ax1.set_ylabel('Fare')\n",
    "ax1.set_title('Linear Regression: Pclass vs Fare')\n",
    "ax1.set_xticks([1, 2, 3])\n",
    "ax1.set_xticklabels(['First', 'Second', 'Third'])\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# Add R² value to plot\n",
    "ax1.text(0.05, 0.95, f\"R² = {r2_score(y_pclass_test, y_pred_pclass):.4f}\", \n",
    "         transform=ax1.transAxes,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "# Plot 2: Pclass and Family Size (3D plot)\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(X_combined_test['pclass'], X_combined_test['family_size'], y_combined_test, \n",
    "           color='blue', alpha=0.6, marker='o', s=40, label='Actual')\n",
    "ax2.scatter(X_combined_test['pclass'], X_combined_test['family_size'], y_pred_combined, \n",
    "           color='red', alpha=0.6, marker='^', s=40, label='Predicted')\n",
    "\n",
    "ax2.set_xlabel('Passenger Class')\n",
    "ax2.set_ylabel('Family Size')\n",
    "ax2.set_zlabel('Fare')\n",
    "ax2.set_title('Linear Regression: Pclass & Family Size vs Fare')\n",
    "ax2.set_xticks([1, 2, 3])\n",
    "ax2.set_xticklabels(['First', 'Second', 'Third'])\n",
    "ax2.view_init(elev=20, azim=-35)\n",
    "\n",
    "# Add R² value to plot\n",
    "ax2.text2D(0.05, 0.95, f\"R² = {r2_score(y_combined_test, y_pred_combined):.4f}\", \n",
    "         transform=ax2.transAxes,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print performance metrics for comparison\n",
    "print(\"Pclass Only Model:\")\n",
    "print(f\"R²: {r2_score(y_pclass_test, y_pred_pclass):.4f}\")\n",
    "print(f\"RMSE: {root_mean_squared_error(y_pclass_test, y_pred_pclass):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_pclass_test, y_pred_pclass):.2f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Pclass + Family Size Model:\")\n",
    "print(f\"R²: {r2_score(y_combined_test, y_pred_combined):.4f}\")\n",
    "print(f\"RMSE: {root_mean_squared_error(y_combined_test, y_pred_combined):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_combined_test, y_pred_combined):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5d621",
   "metadata": {},
   "source": [
    "Interesting. Adding this complexity notably increased the R^2 value, from 0.3361 on the Polynomial regression of p-cl to 0.3852. What if we test degrees 3, 4, 6, and 8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ad203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple polynomial degrees with combined features\n",
    "degrees = [3, 4, 6, 8]\n",
    "poly_models_combined = {}\n",
    "y_preds_combined = {}\n",
    "r2_scores_combined = {}\n",
    "\n",
    "# Define distinct colors for better contrast\n",
    "colors = ['orange', 'green', 'purple', 'red']\n",
    "\n",
    "# Train models for each degree\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly_transformer = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly_deg = poly_transformer.fit_transform(X_combined_train)\n",
    "    X_test_poly_deg = poly_transformer.transform(X_combined_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly_deg, y_combined_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test_poly_deg)\n",
    "    \n",
    "    # Store results\n",
    "    poly_models_combined[degree] = model\n",
    "    y_preds_combined[degree] = y_pred\n",
    "    r2_scores_combined[degree] = r2_score(y_combined_test, y_pred)\n",
    "\n",
    "# Create plots for each degree using 3D visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # Create 3D subplot\n",
    "    ax = plt.subplot(2, 2, i+1, projection='3d')\n",
    "    \n",
    "    # Plot actual data points\n",
    "    ax.scatter(X_combined_test['pclass'], X_combined_test['family_size'], y_combined_test, \n",
    "               color='blue', alpha=0.5, marker='o', s=30, label='Actual')\n",
    "    \n",
    "    # Plot predicted data points\n",
    "    ax.scatter(X_combined_test['pclass'], X_combined_test['family_size'], y_preds_combined[degree], \n",
    "               color=colors[i], alpha=0.7, marker='^', s=40, label=f'Degree {degree}')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Passenger Class')\n",
    "    ax.set_ylabel('Family Size')\n",
    "    ax.set_zlabel('Fare')\n",
    "    ax.set_title(f\"Polynomial (Degree {degree}): Pclass & Family Size vs Fare\")\n",
    "    ax.set_xticks([1, 2, 3])\n",
    "    ax.set_xticklabels(['First', 'Second', 'Third'])\n",
    "    \n",
    "    # Set a lower viewing angle\n",
    "    ax.view_init(elev=20, azim=-35)  # Lower elevation, adjust azimuth\n",
    "    \n",
    "    # Add R² value to plot\n",
    "    ax.text2D(0.05, 0.95, f\"R² = {r2_scores_combined[degree]:.4f}\", \n",
    "             transform=ax.transAxes,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.15, hspace=0.2)  # Adjust spacing\n",
    "plt.show()\n",
    "\n",
    "# Report performance metrics for each model\n",
    "print(\"Performance Metrics for Different Polynomial Degrees (Combined Features):\\n\")\n",
    "for i, degree in enumerate(degrees):\n",
    "    print(f\"Polynomial Degree {degree} ({colors[i]}):\")\n",
    "    print(f\"R²: {r2_scores_combined[degree]:.4f}\")\n",
    "    print(f\"RMSE: {root_mean_squared_error(y_combined_test, y_preds_combined[degree]):.2f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_combined_test, y_preds_combined[degree]):.2f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4aa1b",
   "metadata": {},
   "source": [
    "The decreasing R² value as the polynomial degree increases (from degree 4 to degrees 6 and 8) is a classic sign of overfitting.\n",
    "\n",
    "With degree 4 (R² = 0.5187), the model has found a good balance between complexity and generalization. It captures the underlying patterns in the data without fitting to noise.\n",
    "As we increase to degree 6 (R² = 0.5025), the model becomes more complex but actually performs worse on test data. This suggests it's starting to fit to random fluctuations in the training data.\n",
    "By degree 8 (R² = 0.4912), the overfitting is even more pronounced, with further decreased performance.\n",
    "\n",
    "This pattern is a textbook example of the bias-variance tradeoff in machine learning:\n",
    "Too simple (low degree): high bias, underfitting\n",
    "Good balance (degree 4): captures real patterns \n",
    "Too complex (higher degrees): high variance, overfitting\n",
    "\n",
    "This is why it's generally good practice to try multiple model complexities and select the one that performs best on validation/test data rather than just assuming that more complexity will always yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890503d4",
   "metadata": {},
   "source": [
    "#### Elastic Net regression model:\n",
    "\n",
    "Let's also look at optimizing the Elastic Net Regression. The functions below provide a few approaches to find the best values for Elastic Net Regression.\n",
    "\n",
    "- Feature Transformation with Elastic Net: Tests polynomial feature transformations of degrees 1-5 to find the optimal complexity level while applying Elastic Net regularization.\n",
    "- Target Transformation with Elastic Net: Applies a log transformation to the fare target variable before modeling, which can help with skewed fare distributions.\n",
    "- Elastic Net with Custom Features: Creates domain-specific engineered features like pclass_squared, family_size_squared, and interaction terms between variables to better capture relationships.\n",
    "- Robust Elastic Net with Hyperparameter Fine-Tuning: Uses a more exhaustive grid search over alpha and l1_ratio hyperparameters to find the optimal regularization balance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fb63e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ALL OPTIMIZATION METHODS FOR ELASTIC NET...\n",
      "\n",
      "=== METHOD 1: FEATURE TRANSFORMATION WITH ELASTIC NET ===\n",
      "Degree 1: Best params = {'elasticnet__alpha': 0.1, 'elasticnet__l1_ratio': 0.3}, Test R² = 0.4030\n",
      "Degree 2: Best params = {'elasticnet__alpha': 0.001, 'elasticnet__l1_ratio': 0.3}, Test R² = 0.5127\n",
      "Degree 3: Best params = {'elasticnet__alpha': 0.001, 'elasticnet__l1_ratio': 0.9}, Test R² = 0.5094\n",
      "Degree 4: Best params = {'elasticnet__alpha': 0.001, 'elasticnet__l1_ratio': 0.9}, Test R² = 0.5129\n",
      "Degree 5: Best params = {'elasticnet__alpha': 0.001, 'elasticnet__l1_ratio': 0.9}, Test R² = 0.5170\n",
      "\n",
      "Best feature transformation: Polynomial degree 5\n",
      "Best alpha: 0.001\n",
      "Best l1_ratio: 0.9\n",
      "Test R²: 0.5170\n",
      "Test RMSE: 26.4336\n",
      "\n",
      "=== METHOD 2: TARGET TRANSFORMATION WITH ELASTIC NET ===\n",
      "Best params = {'regressor__elasticnet__alpha': 0.001, 'regressor__elasticnet__l1_ratio': 0.9}\n",
      "Train R² = 0.5014\n",
      "Test R² = 0.5532\n",
      "Test RMSE = 25.4237\n",
      "\n",
      "=== METHOD 3: ELASTIC NET WITH CUSTOM FEATURES ===\n",
      "Best params = {'elasticnet__alpha': 0.001, 'elasticnet__l1_ratio': 0.3}\n",
      "Train R² = 0.5001\n",
      "Test R² = 0.5127\n",
      "Test RMSE = 26.5512\n",
      "\n",
      "Feature Importance:\n",
      "pclass: -98.6070\n",
      "pclass_squared: 88.3156\n",
      "pclass_family_interaction: -59.2116\n",
      "family_size: 56.7508\n",
      "family_size_squared: 10.4174\n",
      "\n",
      "Updating Method 4 with best degree from Method 1: 5\n",
      "\n",
      "=== METHOD 4: ROBUST ELASTIC NET WITH HYPERPARAMETER FINE-TUNING ===\n",
      "Best params = {'elasticnet__alpha': np.float64(0.0011288378916846883), 'elasticnet__l1_ratio': np.float64(0.16473684210526315)}\n",
      "Train R² = 0.5124\n",
      "Test R² = 0.5136\n",
      "Test RMSE = 26.5279\n",
      "\n",
      "=== RESULTS SUMMARY ===\n",
      "                    Method  Train R²  Test R²  Test RMSE    Alpha  Poly Degree  L1 Ratio\n",
      "      Log-Transform Target  0.501407 0.553243  25.423658 0.001000          NaN  0.900000\n",
      "       Polynomial Features  0.516566 0.517045  26.433581 0.001000          5.0  0.900000\n",
      "Fine-Tuned Hyperparameters  0.512419 0.513591  26.527923 0.001129          NaN  0.164737\n",
      "           Custom Features  0.500145 0.512737  26.551214 0.001000          NaN  0.300000\n",
      "\n",
      "Best Method: Log-Transform Target with R² = 0.5532\n",
      "\n",
      "Final Model Visualization (Best Method)\n"
     ]
    }
   ],
   "source": [
    "# Load the Titanic dataset\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Basic data preparation\n",
    "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# Extract features and target\n",
    "X = titanic[['pclass', 'family_size']]\n",
    "y = titanic['fare']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")\n",
    "\n",
    "# Method 1: Feature Transformation with Elastic Net\n",
    "def optimize_feature_transformation():\n",
    "    \"\"\"\n",
    "    Optimize polynomial degree transformation with Elastic Net\n",
    "    \"\"\"\n",
    "    print(\"\\n=== METHOD 1: FEATURE TRANSFORMATION WITH ELASTIC NET ===\")\n",
    "    \n",
    "    degrees = list(range(1, 6))  # Try polynomial degrees 1-5\n",
    "    alphas = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "    l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for degree in degrees:\n",
    "        # Set up a pipeline with polynomial features and elastic net\n",
    "        pipeline = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('elasticnet', ElasticNet(max_iter=10000, tol=1e-4))\n",
    "        ])\n",
    "        \n",
    "        # Set up parameter grid\n",
    "        param_grid = {\n",
    "            'elasticnet__alpha': alphas,\n",
    "            'elasticnet__l1_ratio': l1_ratios\n",
    "        }\n",
    "        \n",
    "        # Use 5-fold cross-validation\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Grid search with cross-validation\n",
    "        grid = GridSearchCV(\n",
    "            pipeline, param_grid, cv=cv, scoring='r2', n_jobs=-1, verbose=0\n",
    "        )\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Get results\n",
    "        y_pred = grid.predict(X_test)\n",
    "        test_r2 = r2_score(y_test, y_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        results.append({\n",
    "            'degree': degree,\n",
    "            'best_alpha': grid.best_params_['elasticnet__alpha'],\n",
    "            'best_l1_ratio': grid.best_params_['elasticnet__l1_ratio'],\n",
    "            'train_r2': grid.best_score_,\n",
    "            'test_r2': test_r2,\n",
    "            'test_rmse': test_rmse\n",
    "        })\n",
    "        \n",
    "        print(f\"Degree {degree}: Best params = {grid.best_params_}, Test R² = {test_r2:.4f}\")\n",
    "        \n",
    "    # Find best result\n",
    "    best_result = max(results, key=lambda x: x['test_r2'])\n",
    "    print(f\"\\nBest feature transformation: Polynomial degree {best_result['degree']}\")\n",
    "    print(f\"Best alpha: {best_result['best_alpha']}\")\n",
    "    print(f\"Best l1_ratio: {best_result['best_l1_ratio']}\")\n",
    "    print(f\"Test R²: {best_result['test_r2']:.4f}\")\n",
    "    print(f\"Test RMSE: {best_result['test_rmse']:.4f}\")\n",
    "    \n",
    "    return best_result\n",
    "\n",
    "\n",
    "# Method 2: Log Transform Target with Elastic Net\n",
    "def optimize_target_transformation():\n",
    "    \"\"\"\n",
    "    Try log transformation of the target variable (fare)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== METHOD 2: TARGET TRANSFORMATION WITH ELASTIC NET ===\")\n",
    "    \n",
    "    # Create log-transformed target pipeline\n",
    "    log_pipe = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('elasticnet', ElasticNet(max_iter=10000, tol=1e-4))\n",
    "    ])\n",
    "    \n",
    "    # Target transformer using natural log (handles positive values only)\n",
    "    # The fare values should all be positive, so this should work\n",
    "    target_transformer = TransformedTargetRegressor(\n",
    "        regressor=log_pipe,\n",
    "        func=np.log1p,  # log(1+x) to handle zero values\n",
    "        inverse_func=lambda x: np.expm1(x)  # exp(x)-1\n",
    "    )\n",
    "    \n",
    "    # Set up parameter grid\n",
    "    param_grid = {\n",
    "        'regressor__elasticnet__alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'regressor__elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    \n",
    "    # Use 5-fold cross-validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid = GridSearchCV(\n",
    "        target_transformer, param_grid, cv=cv, scoring='r2', n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Get results\n",
    "    y_pred = grid.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    print(f\"Best params = {grid.best_params_}\")\n",
    "    print(f\"Train R² = {grid.best_score_:.4f}\")\n",
    "    print(f\"Test R² = {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE = {test_rmse:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'best_alpha': grid.best_params_['regressor__elasticnet__alpha'],\n",
    "        'best_l1_ratio': grid.best_params_['regressor__elasticnet__l1_ratio'],\n",
    "        'train_r2': grid.best_score_,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse\n",
    "    }\n",
    "\n",
    "\n",
    "# Method 3: Elastic Net with Custom Features\n",
    "def optimize_custom_features():\n",
    "    \"\"\"\n",
    "    Create custom features based on domain knowledge\n",
    "    \"\"\"\n",
    "    print(\"\\n=== METHOD 3: ELASTIC NET WITH CUSTOM FEATURES ===\")\n",
    "    \n",
    "    # Create a temporary dataframe for feature engineering\n",
    "    df = titanic.copy()\n",
    "    \n",
    "    # Create custom features\n",
    "    df['fare_per_person'] = df['fare'] / df['family_size']\n",
    "    df['pclass_squared'] = df['pclass'] ** 2\n",
    "    df['family_size_squared'] = df['family_size'] ** 2\n",
    "    df['pclass_family_interaction'] = df['pclass'] * df['family_size']\n",
    "    \n",
    "    # Select features\n",
    "    custom_X = df[['pclass', 'family_size', 'pclass_squared', \n",
    "                   'family_size_squared', 'pclass_family_interaction']]\n",
    "    \n",
    "    # Drop rows with infinite or missing values from feature engineering\n",
    "    mask = np.isfinite(custom_X).all(axis=1)\n",
    "    custom_X = custom_X[mask]\n",
    "    custom_y = df['fare'][mask]\n",
    "    \n",
    "    # Split the data again\n",
    "    X_train_custom, X_test_custom, y_train_custom, y_test_custom = train_test_split(\n",
    "        custom_X, custom_y, test_size=0.2, random_state=123\n",
    "    )\n",
    "    \n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('elasticnet', ElasticNet(max_iter=10000, tol=1e-4))\n",
    "    ])\n",
    "    \n",
    "    # Parameter grid\n",
    "    param_grid = {\n",
    "        'elasticnet__alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    \n",
    "    # Use 5-fold cross-validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Grid search\n",
    "    grid = GridSearchCV(\n",
    "        pipeline, param_grid, cv=cv, scoring='r2', n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train_custom, y_train_custom)\n",
    "    \n",
    "    # Get results\n",
    "    y_pred = grid.predict(X_test_custom)\n",
    "    test_r2 = r2_score(y_test_custom, y_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_custom, y_pred))\n",
    "    \n",
    "    print(f\"Best params = {grid.best_params_}\")\n",
    "    print(f\"Train R² = {grid.best_score_:.4f}\")\n",
    "    print(f\"Test R² = {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE = {test_rmse:.4f}\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    best_model = grid.best_estimator_.named_steps['elasticnet']\n",
    "    feature_names = custom_X.columns\n",
    "    \n",
    "    # Print coefficients\n",
    "    coef = best_model.coef_\n",
    "    importance = np.abs(coef)\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    for i in indices:\n",
    "        if coef[i] != 0:  # Only show non-zero coefficients\n",
    "            print(f\"{feature_names[i]}: {coef[i]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'best_alpha': grid.best_params_['elasticnet__alpha'],\n",
    "        'best_l1_ratio': grid.best_params_['elasticnet__l1_ratio'],\n",
    "        'train_r2': grid.best_score_,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse\n",
    "    }\n",
    "\n",
    "\n",
    "# Method 4: Robust Elastic Net with Hyperparameter Fine-Tuning\n",
    "def optimize_hyperparam_finetuning():\n",
    "    \"\"\"\n",
    "    Fine-tune hyperparameters with exhaustive search\n",
    "    \"\"\"\n",
    "    print(\"\\n=== METHOD 4: ROBUST ELASTIC NET WITH HYPERPARAMETER FINE-TUNING ===\")\n",
    "    \n",
    "    # Best polynomial degree from method 1\n",
    "    best_degree = 2  # This will be updated based on method 1 results\n",
    "    \n",
    "    # Create a more focused pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=best_degree, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('elasticnet', ElasticNet(max_iter=10000, tol=1e-4))\n",
    "    ])\n",
    "    \n",
    "    # More focused parameter grid with fine-tuned values\n",
    "    param_grid = {\n",
    "        'elasticnet__alpha': np.logspace(-4, 1, 20),  # Fine-grained alpha values\n",
    "        'elasticnet__l1_ratio': np.linspace(0.01, 0.99, 20)  # Fine-grained l1_ratio values\n",
    "    }\n",
    "    \n",
    "    # Use 10-fold cross-validation for more robust estimates\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid = GridSearchCV(\n",
    "        pipeline, param_grid, cv=cv, scoring='r2', n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Get results\n",
    "    y_pred = grid.predict(X_test)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    print(f\"Best params = {grid.best_params_}\")\n",
    "    print(f\"Train R² = {grid.best_score_:.4f}\")\n",
    "    print(f\"Test R² = {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE = {test_rmse:.4f}\")\n",
    "    \n",
    "    # Visualize the results\n",
    "    plot_finetuning_results(grid)\n",
    "    \n",
    "    return {\n",
    "        'best_alpha': grid.best_params_['elasticnet__alpha'],\n",
    "        'best_l1_ratio': grid.best_params_['elasticnet__l1_ratio'],\n",
    "        'train_r2': grid.best_score_,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_finetuning_results(grid):\n",
    "    \"\"\"\n",
    "    Plot the hyperparameter tuning results\n",
    "    \"\"\"\n",
    "    # Extract the results\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "    \n",
    "    # Extract alpha and l1_ratio values\n",
    "    alphas = results['param_elasticnet__alpha'].astype(float)\n",
    "    l1_ratios = results['param_elasticnet__l1_ratio'].astype(float)\n",
    "    scores = results['mean_test_score']\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = pd.DataFrame({\n",
    "        'alpha': alphas,\n",
    "        'l1_ratio': l1_ratios,\n",
    "        'score': scores\n",
    "    })\n",
    "    \n",
    "    # Create unique values list for pivoting\n",
    "    alpha_values = sorted(pivot_table['alpha'].unique())\n",
    "    l1_ratio_values = sorted(pivot_table['l1_ratio'].unique())\n",
    "    \n",
    "    # Create an empty matrix\n",
    "    heat_matrix = np.zeros((len(l1_ratio_values), len(alpha_values)))\n",
    "    \n",
    "    # Fill the matrix\n",
    "    for i, l1 in enumerate(l1_ratio_values):\n",
    "        for j, alpha in enumerate(alpha_values):\n",
    "            mask = (pivot_table['alpha'] == alpha) & (pivot_table['l1_ratio'] == l1)\n",
    "            if mask.sum() > 0:\n",
    "                heat_matrix[i, j] = pivot_table.loc[mask, 'score'].values[0]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heat_matrix, \n",
    "                xticklabels=[f\"{a:.2e}\" for a in alpha_values],\n",
    "                yticklabels=[f\"{l:.2f}\" for l in l1_ratio_values],\n",
    "                cmap='viridis', annot=False)\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.ylabel('L1 Ratio')\n",
    "    plt.title('Cross-Validated R² Score by Hyperparameter Combination')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Find and mark the best combination\n",
    "    best_alpha = grid.best_params_['elasticnet__alpha']\n",
    "    best_l1_ratio = grid.best_params_['elasticnet__l1_ratio']\n",
    "    \n",
    "    alpha_idx = alpha_values.index(best_alpha)\n",
    "    l1_idx = l1_ratio_values.index(best_l1_ratio)\n",
    "    \n",
    "    plt.plot(alpha_idx + 0.5, l1_idx + 0.5, 'r*', markersize=15)\n",
    "    plt.annotate('Best', (alpha_idx + 0.5, l1_idx + 0.5), \n",
    "                 xytext=(10, 10), textcoords='offset points',\n",
    "                 color='white', fontsize=12,\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='red', alpha=0.7))\n",
    "    \n",
    "    plt.savefig('elasticnet_hyperparameter_tuning.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_all_methods():\n",
    "    \"\"\"\n",
    "    Run all optimization methods and compare results\n",
    "    \"\"\"\n",
    "    print(\"RUNNING ALL OPTIMIZATION METHODS FOR ELASTIC NET...\")\n",
    "    \n",
    "    method1_result = optimize_feature_transformation()\n",
    "    method2_result = optimize_target_transformation()\n",
    "    method3_result = optimize_custom_features()\n",
    "    \n",
    "    # Update the best degree for method 4 based on method 1 results\n",
    "    best_degree = method1_result['degree']\n",
    "    print(f\"\\nUpdating Method 4 with best degree from Method 1: {best_degree}\")\n",
    "    \n",
    "    # Update the polynomial degree in method 4\n",
    "    method4_result = optimize_hyperparam_finetuning()\n",
    "    \n",
    "    # Compile all results\n",
    "    results = [\n",
    "        {'Method': 'Polynomial Features', **method1_result},\n",
    "        {'Method': 'Log-Transform Target', **method2_result},\n",
    "        {'Method': 'Custom Features', **method3_result},\n",
    "        {'Method': 'Fine-Tuned Hyperparameters', **method4_result}\n",
    "    ]\n",
    "    \n",
    "    # Create a DataFrame to display results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Reorder and select columns for display\n",
    "    display_columns = ['Method', 'train_r2', 'test_r2', 'test_rmse', \n",
    "                       'best_alpha', 'best_l1_ratio']\n",
    "    \n",
    "    # Add degree column for method 1\n",
    "    if 'degree' in results_df.columns:\n",
    "        display_columns.insert(5, 'degree')\n",
    "    \n",
    "    # Reindex and display only selected columns\n",
    "    display_df = results_df[display_columns].sort_values(by='test_r2', ascending=False)\n",
    "    display_df = display_df.rename(columns={\n",
    "        'train_r2': 'Train R²',\n",
    "        'test_r2': 'Test R²',\n",
    "        'test_rmse': 'Test RMSE',\n",
    "        'best_alpha': 'Alpha',\n",
    "        'best_l1_ratio': 'L1 Ratio',\n",
    "        'degree': 'Poly Degree'\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== RESULTS SUMMARY ===\")\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Identify the best method\n",
    "    best_method = display_df.iloc[0]['Method']\n",
    "    best_r2 = display_df.iloc[0]['Test R²']\n",
    "    \n",
    "    print(f\"\\nBest Method: {best_method} with R² = {best_r2:.4f}\")\n",
    "    \n",
    "    return display_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = evaluate_all_methods()\n",
    "    \n",
    "    # Visualize final predictions using best method\n",
    "    print(\"\\nFinal Model Visualization (Best Method)\")\n",
    "    \n",
    "    # This is a placeholder for setting up the best model from the results\n",
    "    # In practice, you would use the best parameters identified\n",
    "    # from the optimization methods\n",
    "    \n",
    "    # Example:\n",
    "    # best_method = results.iloc[0]['Method']\n",
    "    # if best_method == 'Polynomial Features':\n",
    "    #     # Set up the model using the best parameters\n",
    "    #     # ...\n",
    "    # ... and so on for other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684d98c",
   "metadata": {},
   "source": [
    "#### Elastic Net Fine Tuning\n",
    "\n",
    "Finally, we can run back-to-back tests to  The elastic_net_fine_tuning function implements a comprehensive grid search across multiple modeling dimensions:\n",
    "\n",
    "- Tests combinations of alpha values, L1 ratios, polynomial degrees (2-5), and both standard/log-transformed targets\n",
    "- Evaluates all parameter combinations and ranks by test R²\n",
    "- Creates visualizations of parameter relationships through heatmaps\n",
    "- Provides detailed evaluation of the best model including feature importance analysis, actual vs. predicted plots, and cross-validation\n",
    "- Supports multiple random seeds to verify reproducibility\n",
    "\n",
    "The function takes ~5 minutes to run; I've included example output at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef096734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "RUNNING WITH RANDOM SEED 456\n",
      "==================================================\n",
      "\n",
      "Starting Elastic Net fine-tuning with random seed: 456\n",
      "Testing 1160 parameter combinations...\n",
      "Progress: 50/1160 combinations tested (4.3%) - Time elapsed: 2.8s\n",
      "Progress: 100/1160 combinations tested (8.6%) - Time elapsed: 5.8s\n",
      "Progress: 150/1160 combinations tested (12.9%) - Time elapsed: 8.9s\n",
      "Progress: 200/1160 combinations tested (17.2%) - Time elapsed: 16.8s\n",
      "Progress: 250/1160 combinations tested (21.6%) - Time elapsed: 24.8s\n",
      "Progress: 300/1160 combinations tested (25.9%) - Time elapsed: 34.6s\n",
      "Progress: 350/1160 combinations tested (30.2%) - Time elapsed: 55.1s\n",
      "Progress: 400/1160 combinations tested (34.5%) - Time elapsed: 75.6s\n",
      "Progress: 450/1160 combinations tested (38.8%) - Time elapsed: 100.4s\n",
      "Progress: 500/1160 combinations tested (43.1%) - Time elapsed: 139.1s\n"
     ]
    }
   ],
   "source": [
    "def elastic_net_fine_tuning(random_seed=456, n_top_results=3):\n",
    "    \"\"\"\n",
    "    Fine-tune elastic net regression with detailed grid search focused on:\n",
    "    - Alpha values from 0.0001 to 0.001\n",
    "    - L1 ratios at both extremes (0.01 and 0.9)\n",
    "    - Log-transformed target for Method 2 (which performed best previously)\n",
    "    \n",
    "    Returns the top n parameter combinations based on test R² score.\n",
    "    \"\"\"\n",
    "    print(f\"Starting Elastic Net fine-tuning with random seed: {random_seed}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Load the Titanic dataset\n",
    "    titanic = sns.load_dataset(\"titanic\")\n",
    "    \n",
    "    # Basic data preparation\n",
    "    titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
    "    titanic = titanic.dropna(subset=['fare'])\n",
    "    titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "    \n",
    "    # Create custom features\n",
    "    titanic['pclass_squared'] = titanic['pclass'] ** 2\n",
    "    titanic['family_size_squared'] = titanic['family_size'] ** 2\n",
    "    titanic['pclass_family_interaction'] = titanic['pclass'] * titanic['family_size']\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = titanic[['pclass', 'family_size', 'pclass_squared', 'family_size_squared', 'pclass_family_interaction']]\n",
    "    y = titanic['fare']\n",
    "    y_log = np.log1p(y)  # Log-transform the target\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test, y_log_train, y_log_test = train_test_split(\n",
    "        X, y, y_log, test_size=0.2, random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    # Define fine-grained alpha values\n",
    "    alpha_values = np.concatenate([\n",
    "        np.linspace(0.0001, 0.001, 20),   # Fine-grained low alpha values\n",
    "        np.linspace(0.001, 0.01, 10)[1:]  # Some slightly higher values\n",
    "    ])\n",
    "    \n",
    "    # Define L1 ratios to test (both extremes)\n",
    "    l1_ratios = [0.01, 0.1, 0.5, 0.9, 0.99]  # Test both near-Ridge and near-Lasso\n",
    "    \n",
    "    # Try both regular and log-transformed targets\n",
    "    target_transforms = [\n",
    "        ('standard', y_train, y_test),\n",
    "        ('log', y_log_train, y_log_test)\n",
    "    ]\n",
    "    \n",
    "    # Try polynomial features of degrees 1-5\n",
    "    poly_degrees = [2, 3, 4, 5]\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_results = []\n",
    "    \n",
    "    # Parameter grid search\n",
    "    total_combinations = len(alpha_values) * len(l1_ratios) * len(target_transforms) * len(poly_degrees)\n",
    "    print(f\"Testing {total_combinations} parameter combinations...\")\n",
    "    \n",
    "    # Progress counter\n",
    "    count = 0\n",
    "    \n",
    "    # Loop through all parameter combinations\n",
    "    for target_name, y_tr, y_te in target_transforms:\n",
    "        for degree in poly_degrees:\n",
    "            # Create the polynomial features transformer\n",
    "            poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "            X_train_poly = poly.fit_transform(X_train)\n",
    "            X_test_poly = poly.transform(X_test)\n",
    "            \n",
    "            for alpha in alpha_values:\n",
    "                for l1_ratio in l1_ratios:\n",
    "                    # Update progress\n",
    "                    count += 1\n",
    "                    if count % 50 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        print(f\"Progress: {count}/{total_combinations} combinations tested ({count/total_combinations*100:.1f}%) - Time elapsed: {elapsed:.1f}s\")\n",
    "                    \n",
    "                    # Create and fit the elastic net model\n",
    "                    model = ElasticNet(\n",
    "                        alpha=alpha,\n",
    "                        l1_ratio=l1_ratio,\n",
    "                        max_iter=10000,\n",
    "                        tol=1e-4,\n",
    "                        random_state=random_seed\n",
    "                    )\n",
    "                    \n",
    "                    # Fit the model\n",
    "                    model.fit(X_train_poly, y_tr)\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    y_pred_train = model.predict(X_train_poly)\n",
    "                    y_pred_test = model.predict(X_test_poly)\n",
    "                    \n",
    "                    # Transform predictions back if using log target\n",
    "                    if target_name == 'log':\n",
    "                        y_pred_train_original = np.expm1(y_pred_train)\n",
    "                        y_pred_test_original = np.expm1(y_pred_test)\n",
    "                        train_r2 = r2_score(y_train, y_pred_train_original)\n",
    "                        test_r2 = r2_score(y_test, y_pred_test_original)\n",
    "                        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_original))\n",
    "                        test_mae = mean_absolute_error(y_test, y_pred_test_original)\n",
    "                    else:\n",
    "                        train_r2 = r2_score(y_train, y_pred_train)\n",
    "                        test_r2 = r2_score(y_test, y_pred_test)\n",
    "                        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "                        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "                    \n",
    "                    # Store results\n",
    "                    all_results.append({\n",
    "                        'target': target_name,\n",
    "                        'degree': degree,\n",
    "                        'alpha': alpha,\n",
    "                        'l1_ratio': l1_ratio,\n",
    "                        'train_r2': train_r2,\n",
    "                        'test_r2': test_r2,\n",
    "                        'test_rmse': test_rmse,\n",
    "                        'test_mae': test_mae\n",
    "                    })\n",
    "    \n",
    "    # Convert to DataFrame for easier sorting and filtering\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Sort by test R²\n",
    "    results_df = results_df.sort_values('test_r2', ascending=False)\n",
    "    \n",
    "    # Get the top n results\n",
    "    top_results = results_df.head(n_top_results)\n",
    "    \n",
    "    # Print elapsed time\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Fine-tuning completed in {elapsed:.1f} seconds\")\n",
    "    \n",
    "    # Print top results\n",
    "    print(\"\\n===== TOP ELASTIC NET RESULTS =====\")\n",
    "    for i, row in top_results.iterrows():\n",
    "        print(f\"Rank {results_df.index.get_loc(i) + 1}:\")\n",
    "        print(f\"  Target Transform: {'Log-transformed' if row['target'] == 'log' else 'Standard'}\")\n",
    "        print(f\"  Polynomial Degree: {row['degree']}\")\n",
    "        print(f\"  Alpha: {row['alpha']:.6f}\")\n",
    "        print(f\"  L1 Ratio: {row['l1_ratio']:.6f}\")\n",
    "        print(f\"  Test R²: {row['test_r2']:.6f}\")\n",
    "        print(f\"  Test RMSE: {row['test_rmse']:.6f}\")\n",
    "        print(f\"  Train R²: {row['train_r2']:.6f}\")\n",
    "        print()\n",
    "    \n",
    "    # Plot alpha vs R² for the best target transformation and degree\n",
    "    best_config = results_df.iloc[0]\n",
    "    best_target = best_config['target']\n",
    "    best_degree = best_config['degree']\n",
    "    \n",
    "    plot_df = results_df[(results_df['target'] == best_target) & (results_df['degree'] == best_degree)]\n",
    "    \n",
    "    plot_alpha_l1_ratio_grid(plot_df, title=f\"R² by Alpha and L1 Ratio ({best_target} target, degree {best_degree})\")\n",
    "    \n",
    "    # Evaluate the best model in more detail\n",
    "    best_alpha = best_config['alpha']\n",
    "    best_l1_ratio = best_config['l1_ratio']\n",
    "    best_target_name = best_config['target']\n",
    "    best_degree = best_config['degree']\n",
    "    \n",
    "    evaluate_best_model(\n",
    "        X_train, X_test, \n",
    "        y_log_train if best_target_name == 'log' else y_train,\n",
    "        y_log_test if best_target_name == 'log' else y_test,\n",
    "        y_train, y_test,  # Original targets for evaluation\n",
    "        best_alpha, best_l1_ratio, best_degree, best_target_name,\n",
    "        random_seed\n",
    "    )\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def plot_alpha_l1_ratio_grid(results_df, title=\"R² by Alpha and L1 Ratio\"):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of R² scores by alpha and L1 ratio\n",
    "    \"\"\"\n",
    "    # Create a pivot table for the heatmap\n",
    "    pivot_df = pd.pivot_table(\n",
    "        results_df,\n",
    "        values='test_r2',\n",
    "        index='l1_ratio',\n",
    "        columns='alpha',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(pivot_df, annot=True, fmt=\".4f\", cmap=\"viridis\", cbar_kws={'label': 'R² Score'})\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.ylabel('L1 Ratio')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('elastic_net_grid_search.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a line plot of R² vs alpha for different L1 ratios\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for l1_ratio in results_df['l1_ratio'].unique():\n",
    "        subset = results_df[results_df['l1_ratio'] == l1_ratio]\n",
    "        plt.plot(subset['alpha'], subset['test_r2'], 'o-', label=f'L1 Ratio = {l1_ratio}')\n",
    "    \n",
    "    plt.title(\"R² Score by Alpha for Different L1 Ratios\")\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('elastic_net_alpha_lines.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_best_model(X_train, X_test, y_train, y_test, y_orig_train, y_orig_test, \n",
    "                        best_alpha, best_l1_ratio, best_degree, best_target, random_seed):\n",
    "    \"\"\"\n",
    "    Evaluate the best model in more detail\n",
    "    \"\"\"\n",
    "    print(\"\\n===== DETAILED EVALUATION OF BEST MODEL =====\")\n",
    "    print(f\"Target Transform: {'Log-transformed' if best_target == 'log' else 'Standard'}\")\n",
    "    print(f\"Polynomial Degree: {best_degree}\")\n",
    "    print(f\"Alpha: {best_alpha:.6f}\")\n",
    "    print(f\"L1 Ratio: {best_l1_ratio:.6f}\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    # Create and fit the model\n",
    "    model = ElasticNet(\n",
    "        alpha=best_alpha,\n",
    "        l1_ratio=best_l1_ratio,\n",
    "        max_iter=10000,\n",
    "        tol=1e-4,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_poly)\n",
    "    y_pred_test = model.predict(X_test_poly)\n",
    "    \n",
    "    # Transform predictions back if using log target\n",
    "    if best_target == 'log':\n",
    "        y_pred_train_original = np.expm1(y_pred_train)\n",
    "        y_pred_test_original = np.expm1(y_pred_test)\n",
    "    else:\n",
    "        y_pred_train_original = y_pred_train\n",
    "        y_pred_test_original = y_pred_test\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_orig_train, y_pred_train_original)\n",
    "    test_r2 = r2_score(y_orig_test, y_pred_test_original)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_orig_test, y_pred_test_original))\n",
    "    test_mae = mean_absolute_error(y_orig_test, y_pred_test_original)\n",
    "    \n",
    "    print(f\"Train R²: {train_r2:.6f}\")\n",
    "    print(f\"Test R²: {test_r2:.6f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.6f}\")\n",
    "    print(f\"Test MAE: {test_mae:.6f}\")\n",
    "    \n",
    "    # Check for feature importance\n",
    "    feature_names = poly.get_feature_names_out(['pclass', 'family_size', 'pclass_squared', \n",
    "                                              'family_size_squared', 'pclass_family_interaction'])\n",
    "    \n",
    "    # Get coefficients and sort by absolute value\n",
    "    coefs = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': model.coef_\n",
    "    })\n",
    "    coefs['abs_coef'] = coefs['coefficient'].abs()\n",
    "    coefs = coefs.sort_values('abs_coef', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop Feature Coefficients:\")\n",
    "    print(coefs[['feature', 'coefficient']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_orig_test, y_pred_test_original, alpha=0.5)\n",
    "    plt.plot([y_orig_test.min(), y_orig_test.max()], [y_orig_test.min(), y_orig_test.max()], 'r--')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = y_orig_test - y_pred_test_original\n",
    "    plt.scatter(y_pred_test_original, residuals, alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('elastic_net_best_model_evaluation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Cross-validation to check robustness\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "    \n",
    "    # Create a pipeline for cross-validation\n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=best_degree, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('elasticnet', ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio, max_iter=10000, random_state=random_seed))\n",
    "    ])\n",
    "    \n",
    "    # If using log target, create a transformed target regressor\n",
    "    if best_target == 'log':\n",
    "        regressor = TransformedTargetRegressor(\n",
    "            regressor=pipeline,\n",
    "            func=np.log1p,\n",
    "            inverse_func=np.expm1\n",
    "        )\n",
    "        cv_scores = cross_val_score(regressor, X_train, y_orig_train, cv=cv, scoring='r2')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='r2')\n",
    "    \n",
    "    print(\"\\nCross-Validation R² Scores:\")\n",
    "    print(f\"Mean: {cv_scores.mean():.6f}\")\n",
    "    print(f\"Std Dev: {cv_scores.std():.6f}\")\n",
    "    print(f\"Min: {cv_scores.min():.6f}\")\n",
    "    print(f\"Max: {cv_scores.max():.6f}\")\n",
    "    print(f\"Individual scores: {cv_scores}\")\n",
    "    \n",
    "    return model, poly\n",
    "\n",
    "\n",
    "# Run the fine-tuning with different random seeds\n",
    "def run_multiple_seeds(seeds=[123], n_top_results=3):\n",
    "    \"\"\"\n",
    "    Run the elastic net fine-tuning with multiple random seeds\n",
    "    to check reproducibility\n",
    "    \"\"\"\n",
    "    all_top_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n\\n{'='*50}\")\n",
    "        print(f\"RUNNING WITH RANDOM SEED {seed}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        results = elastic_net_fine_tuning(random_seed=seed, n_top_results=n_top_results)\n",
    "        top_results = results.head(n_top_results).copy()\n",
    "        top_results['seed'] = seed\n",
    "        all_top_results.append(top_results)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_results = pd.concat(all_top_results)\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"SUMMARY OF TOP RESULTS ACROSS ALL RANDOM SEEDS\")\n",
    "    print(\"=\"*80)\n",
    "    print(combined_results[['seed', 'target', 'degree', 'alpha', 'l1_ratio', 'test_r2', 'train_r2']].to_string())\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run with a single seed\n",
    "    #results = elastic_net_fine_tuning(random_seed=123, n_top_results=3)\n",
    "    \n",
    "    # Run with multiple seeds to check reproducibility\n",
    "    combined_results = run_multiple_seeds(seeds=[456], n_top_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1ddd6",
   "metadata": {},
   "source": [
    "This code can take a while to run; Here's some example output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''===== TOP ELASTIC NET RESULTS =====\n",
    "Rank 1:\n",
    "  Target Transform: Log-transformed\n",
    "  Polynomial Degree: 3\n",
    "  Alpha: 0.000100\n",
    "  L1 Ratio: 0.990000\n",
    "  Test R²: 0.571709\n",
    "  Test RMSE: 24.892703\n",
    "  Train R²: 0.442862\n",
    "\n",
    "Rank 2:\n",
    "  Target Transform: Log-transformed\n",
    "  Polynomial Degree: 3\n",
    "  Alpha: 0.000100\n",
    "  L1 Ratio: 0.900000\n",
    "  Test R²: 0.571698\n",
    "  Test RMSE: 24.893007\n",
    "  Train R²: 0.442850\n",
    "\n",
    "Rank 3:\n",
    "  Target Transform: Log-transformed\n",
    "  Polynomial Degree: 3\n",
    "  Alpha: 0.000100\n",
    "  L1 Ratio: 0.500000\n",
    "  Test R²: 0.571655\n",
    "  Test RMSE: 24.894280\n",
    "  Train R²: 0.442800'''\n",
    "\n",
    "\n",
    "'''\n",
    "===== TOP ELASTIC NET RESULTS =====\n",
    "Rank 1:\n",
    "  Target Transform: Standard\n",
    "  Polynomial Degree: 4\n",
    "  Alpha: 0.000100\n",
    "  L1 Ratio: 0.990000\n",
    "  Test R²: 0.540643\n",
    "  Test RMSE: 25.795236\n",
    "  Train R²: 0.482978\n",
    "\n",
    "Rank 2:\n",
    "  Target Transform: Standard\n",
    "  Polynomial Degree: 4\n",
    "  Alpha: 0.000147\n",
    "  L1 Ratio: 0.990000\n",
    "  Test R²: 0.540642\n",
    "  Test RMSE: 25.795258\n",
    "  Train R²: 0.482978\n",
    "\n",
    "Rank 3:\n",
    "  Target Transform: Standard\n",
    "  Polynomial Degree: 4\n",
    "  Alpha: 0.000195\n",
    "  L1 Ratio: 0.990000\n",
    "  Test R²: 0.540642\n",
    "  Test RMSE: 25.795280\n",
    "  Train R²: 0.482978\n",
    "\n",
    "\n",
    "===== DETAILED EVALUATION OF BEST MODEL =====\n",
    "Target Transform: Standard\n",
    "Polynomial Degree: 4\n",
    "Alpha: 0.000100\n",
    "L1 Ratio: 0.990000\n",
    "Train R²: 0.482978\n",
    "Test R²: 0.540643\n",
    "Test RMSE: 25.795236\n",
    "Test MAE: 12.322013\n",
    "\n",
    "Top Feature Coefficients:\n",
    "                              feature  coefficient\n",
    "                               pclass   -76.606712\n",
    "                          family_size    26.988818\n",
    "            pclass_family_interaction    -4.103641\n",
    "                       pclass_squared     3.079243\n",
    "                             pclass^2     2.667246\n",
    "                        family_size^2     2.598875\n",
    "           family_size pclass_squared    -2.216267\n",
    "family_size pclass_family_interaction    -1.658725\n",
    "     pclass pclass_family_interaction     1.377558\n",
    "                 pclass^2 family_size     1.371299\n",
    "\n",
    "Cross-Validation R² Scores:\n",
    "Mean: 0.506149\n",
    "Std Dev: 0.157643\n",
    "Min: 0.255964\n",
    "Max: 0.655539\n",
    "Individual scores: [0.65433923 0.25596388 0.57262717 0.39227503 0.65553852]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb704de9",
   "metadata": {},
   "source": [
    "### Section 6. Final Thoughts & Insights\n",
    "\n",
    "6.1 Summarize Findings\n",
    "What features were most useful?\n",
    "- Pclass + family size produced the best study and lowest values from those that I tested.\n",
    "\n",
    "\n",
    "What regression model performed best?\n",
    "I had very good luck with Polynomial, particularly for an index of 5. Past that it seemed to overfit.\n",
    "\n",
    "While Elastic Net did great on the Test data, it's worse performance on the training data is suspicious. Possibly worth additional investigation.\n",
    "\n",
    "\n",
    "How did model complexity or regularization affect results?\n",
    "Adding Family size to Pclass greatly increased our ability to increase R^2, up past 0.5, where as before with just pclass it hovered around 0.3.\n",
    "Regularization techniques, particularly ElasticNet, helped the model avoid overfitting to outliers in the first class, allowing it to create a more balanced fit across all classes.\n",
    " \n",
    "\n",
    "6.2 Discuss Challenges\n",
    "Was fare hard to predict? Why?\n",
    "Yes. It has almost no correlation with most of the other features on the table. Even the best models only got us up to an R^2 around 0.5, and most single features only got us 0.05. \n",
    "\n",
    "\n",
    "Did skew or outliers impact the models?\n",
    "Yes indeed. The fare distribution was heavily skewed, which is why log transformation significantly improved model performance. The notebook specifically mentions that \"first class passengers where there's tremendous fare variability\" created challenges. \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
